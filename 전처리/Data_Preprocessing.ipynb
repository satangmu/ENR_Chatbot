{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SmartNurse/Year_Dream/blob/main/BaseLine_(2023_11_21).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oedssRYvckZ8"
      },
      "source": [
        "# 라이브러리 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7YyLsRQg5lZ",
        "outputId": "3d14d5fd-e357-4021-c764-42855cb1f8b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00KkESbeIrkm"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wd2oHeacnY9"
      },
      "source": [
        "# 데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgd98dEwI6-P"
      },
      "outputs": [],
      "source": [
        "# ncp JSON 파일 열기\n",
        "ncp_filename = '/content/drive/MyDrive/Colab Notebooks/data/Smart_Nurse2/nursingrecord_ncp.json'\n",
        "with open(ncp_filename, 'r') as file:\n",
        "    ncp_data = json.load(file)\n",
        "\n",
        "# aws JSON 파일 열기\n",
        "aws_filename = '/content/drive/MyDrive/Colab Notebooks/data/Smart_Nurse2/nursingrecord_aws.json'\n",
        "with open(aws_filename, 'r') as file:\n",
        "    aws_data = json.load(file)\n",
        "\n",
        "info_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/data/Smart_Nurse2/patientinfo.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdf5kUD9_zMF"
      },
      "outputs": [],
      "source": [
        "# JSON 문자열을 파이썬 딕셔너리로 변환하는 함수\n",
        "def parse_json(json_str):\n",
        "    try:\n",
        "        return json.loads(json_str)\n",
        "    except json.JSONDecodeError:\n",
        "        return {}  # JSON 파싱 에러 시 빈 딕셔너리 반환\n",
        "\n",
        "# 각 레코드에 대해 'content' 필드를 파싱\n",
        "for item in ncp_data:\n",
        "    item['content'] = parse_json(item['content'])\n",
        "\n",
        "for item in aws_data:\n",
        "    item['content'] = parse_json(item['content'])\n",
        "\n",
        "\n",
        "# 데이터프레임 생성\n",
        "ncp_df = pd.DataFrame(ncp_data)\n",
        "aws_df = pd.DataFrame(aws_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNaqAtfWcu5r"
      },
      "source": [
        "# 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdDKS1kPLQs7"
      },
      "outputs": [],
      "source": [
        "NANDA_columns = ['domain', 'class', 'diagnosis', 'collectingData', 'goal', 'plan', 'interventions', 'evaluation']\n",
        "SOAPIE_columns = ['subjective', 'objective', 'assessment', 'planning', 'interventions', 'evaluation']\n",
        "FOCUS_DAR_columns = ['focus', 'data', 'action', 'response']\n",
        "NARRATIVE_NOTES_columns = ['narrativeNote']\n",
        "NURSING_columns = ['assessment', 'diagnosisRelate', 'diagnosis', 'goal', 'plan', 'interventions', 'evaluation']\n",
        "\n",
        "def pre_processing(ncp_df, aws_df, record_type, columns)  :\n",
        "    # 특정 'record_type' 행들만 필터링\n",
        "    record_type_dict = {'NANDA' : 0, 'SOAPIE' : 1, 'FOCUS_DAR' : 2, 'NARRATIVE_NOTES' : 3, 'NURSING' : 4}\n",
        "    ncp_record_type = ncp_df[ncp_df['record_type'] == record_type_dict[record_type]]\n",
        "    aws_record_type = aws_df[aws_df['record_type'] == record_type_dict[record_type]]\n",
        "\n",
        "    # 'content' 열을 확장하여 새로운 데이터 프레임 생성\n",
        "    ncp_content_df = pd.json_normalize(ncp_record_type['content'])\n",
        "    aws_content_df = pd.json_normalize(aws_record_type['content'])\n",
        "\n",
        "    # 필요한 열만 가져오기\n",
        "    ncp_content_df = ncp_content_df[columns]\n",
        "    aws_content_df = aws_content_df[columns]\n",
        "\n",
        "    # 두 데이터프레임 합치기\n",
        "    df = pd.concat([ncp_content_df, aws_content_df])\n",
        "\n",
        "    # 중복된 행 제거\n",
        "    df = df.drop_duplicates()\n",
        "    return df\n",
        "\n",
        "# 전처리 한 데이터프레임 불러오기\n",
        "nanda_df = pre_processing(ncp_df, aws_df, 'NANDA', NANDA_columns)\n",
        "soapie_df = pre_processing(ncp_df, aws_df, 'SOAPIE', SOAPIE_columns)\n",
        "focus_dar_df = pre_processing(ncp_df, aws_df, 'FOCUS_DAR', FOCUS_DAR_columns)\n",
        "narrative_notes_df = pre_processing(ncp_df, aws_df, 'NARRATIVE_NOTES', NARRATIVE_NOTES_columns)\n",
        "nursing_df = pre_processing(ncp_df, aws_df, 'NURSING', NURSING_columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCeD-CBezwbf"
      },
      "source": [
        "## 전처리 함수 모음"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFilWRrbRwte"
      },
      "outputs": [],
      "source": [
        "# 필터링 조건에 맞지 않는 행을 확인하는 함수\n",
        "def is_valid_row(text):\n",
        "    # 숫자만 / 알파벳만 / 한글자모음만 / 특수 문자만 있는 경우\n",
        "    if re.fullmatch(r'\\d+', text) or re.fullmatch(r'[a-zA-Z]+', text) or re.fullmatch(r'[ㄱ-ㅎㅏ-ㅣ]+', text) or re.fullmatch(r'[^\\w\\s]+', text):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# 필터링 조건에 맞지 않는 단어를 제거하는 함수\n",
        "def remove_invalid_words(text):\n",
        "    words = str(text).split()\n",
        "    valid_words = [word for word in words if is_valid_row(word)]\n",
        "    return ' '.join(valid_words) if valid_words else pd.NA  # 빈 문자열 대신 pd.NA 반환\n",
        "\n",
        "# 길이가 n 미만인 데이터가 있는 행 제거\n",
        "def remove_short_words(text):\n",
        "    # 문자열로 변환하고 단어로 분리\n",
        "    words = str(text).split()\n",
        "    # 길이가 3 초과인 단어만 유효하다고 판단\n",
        "    valid_words = [word for word in words if len(word) > 3]\n",
        "    # 유효한 단어들을 다시 문자열로 합치기\n",
        "    return ' '.join(valid_words) if valid_words else pd.NA\n",
        "\n",
        "def unify_terms(sentence):\n",
        "    S_patterns_to_unify = [\n",
        "        r'<주관적(?![\\]자료])',\n",
        "        r'\\[주관적\\]',\n",
        "        r'주관적:',\n",
        "        r'-주관적:',\n",
        "        r'주관적자료:',\n",
        "        r'\\[주관적자료\\]',\n",
        "        r'주관적자료(?!\\])',\n",
        "        r'\\<주관적자료\\>',\n",
        "        r'\\[주관적',\n",
        "        r'\\*주관적',\n",
        "        r'\\(주관적\\)',\n",
        "        r'\\(주관적',\n",
        "        r'주관적-보호자',\n",
        "        r'\\(\\[주관적자료\\]\\)',\n",
        "        r'-\\[주관적자료\\]',\n",
        "        r'\\*\\[주관적자료\\]',\n",
        "    ]\n",
        "\n",
        "    O_patterns_to_unify = [\n",
        "        r'<객관적(?![\\]자료])',\n",
        "        r'\\[객관적\\]',\n",
        "        r'객관적:',\n",
        "        r'-객관적:',\n",
        "        r'객관적자료:',\n",
        "        r'\\[객관적자료\\]',\n",
        "        r'객관적자료(?!\\])',\n",
        "        r'\\<객관적자료\\>',\n",
        "        r'\\[객관적',\n",
        "        r'\\*객관적',\n",
        "        r'\\(객관적\\)',\n",
        "        r'\\(객관적',\n",
        "        r'객관적-보호자',\n",
        "        r'\\(\\[객관적자료\\]\\)',\n",
        "        r'-\\[객관적자료\\]',\n",
        "        r'\\*\\[객관적자료\\]',\n",
        "    ]\n",
        "    ST_patterns_to_unitfy = [\n",
        "        r'단기목표:',\n",
        "        r'\\<단기목표\\>',\n",
        "        r'단기목표',\n",
        "        r'-단기목표',\n",
        "        r'단기',\n",
        "        r'단기:',\n",
        "        r'\\[단기\\]',\n",
        "        r'\\<단기\\>',\n",
        "        r'\\*단기목표:',\n",
        "        r'\\*\\[단기목표\\]',\n",
        "        r'\\*단기:',\n",
        "        r'\\*\\<단기목표\\>',\n",
        "        r'\\*단기목표',\n",
        "        r'\\[단기목표\\]\\)',\n",
        "        r'\\(\\[단기목표\\]\\)'\n",
        "    ]\n",
        "    LT_patterns_to_unitfy = [\n",
        "        r'장기목표:',\n",
        "        r'\\<장기목표\\>',\n",
        "        r'장기목표',\n",
        "        r'-장기목표',\n",
        "        r'장기',\n",
        "        r'장기:',\n",
        "        r'\\[장기\\]',\n",
        "        r'\\<장기\\>',\n",
        "        r'\\*장기목표:',\n",
        "        r'\\*\\[장기목표\\]',\n",
        "        r'\\*장기:',\n",
        "        r'\\*\\<장기목표\\>',\n",
        "        r'\\*장기목표',\n",
        "        r'\\[장기목표\\]\\)',\n",
        "        r'\\(\\[장기목표\\]\\)'\n",
        "\n",
        "    ]\n",
        "\n",
        "\n",
        "    for pattern in S_patterns_to_unify:\n",
        "      sentence = re.sub(pattern, '[주관적자료]', sentence)\n",
        "\n",
        "    for pattern in O_patterns_to_unify:\n",
        "      sentence = re.sub(pattern, '[객관적자료]', sentence)\n",
        "\n",
        "    for pattern in ST_patterns_to_unitfy:\n",
        "      sentence = re.sub(pattern, '[단기목표]', sentence)\n",
        "\n",
        "    for pattern in LT_patterns_to_unitfy:\n",
        "      sentence = re.sub(pattern, '[장기목표]', sentence)\n",
        "\n",
        "\n",
        "    # '[주관적자료]자료]'와 같은 패턴을 '[주관적자료]'로 치환\n",
        "    sentence = re.sub(r'\\[주관적자료\\](자료\\])+', '[주관적자료]', sentence)\n",
        "    sentence = re.sub(r'\\[객관적자료\\](자료\\])+', '[객관적자료]', sentence)\n",
        "    sentence = re.sub(r'\\[\\[\\[단기목표\\]목표\\]\\]', '[단기목표]', sentence)\n",
        "    sentence = re.sub(r'\\[\\[단기목표\\]목표\\]', '[단기목표]', sentence)\n",
        "    sentence = re.sub(r'\\[단기목표\\]:', '[단기목표]', sentence)\n",
        "    sentence = re.sub(r'\\[\\[\\[장기목표\\]목표\\]\\]', '[장기목표]', sentence)\n",
        "    sentence = re.sub(r'\\[\\[장기목표\\]목표\\]', '[장기목표]', sentence)\n",
        "    sentence = re.sub(r'\\[장기목표\\]:', '[장기목표]', sentence)\n",
        "\n",
        "\n",
        "    return sentence\n",
        "\n",
        "def check_row_for_duplicates(row):\n",
        "    row_values = row.tolist()\n",
        "    return len(row_values) != len(set(row_values))\n",
        "\n",
        "#위 전처리 함수들을 적용하는 함수\n",
        "def function_apply(df, filter_column) :\n",
        "    # 모든 열에 적용\n",
        "    for column in df.columns :\n",
        "        df[column] = df[column].apply(remove_invalid_words)\n",
        "\n",
        "    # filter된 열에만 적용\n",
        "    for column in filter_column :\n",
        "        df[column] = df[column].apply(remove_short_words)\n",
        "\n",
        "    # 행별로 적용\n",
        "    df = df[~df.apply(check_row_for_duplicates, axis = 1)]\n",
        "\n",
        "    # '<NA>'가 포함된 행 제거\n",
        "    df = df[~df.apply(lambda x: x.astype(str).str.contains('<NA>')).any(axis=1)]\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "    # 모든 행 동음이의어 처리\n",
        "    for column in df.columns:\n",
        "      df[column] = df[column].apply(unify_terms)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIl17tlOSleu"
      },
      "outputs": [],
      "source": [
        "nanda_filter_column = ['collectingData', 'goal', 'plan', 'interventions', 'evaluation']\n",
        "soapie_filter_column = ['subjective', 'objective', 'assessment', 'planning', 'interventions', 'evaluation']\n",
        "focus_dar_filter_column = ['focus', 'data', 'action','response']\n",
        "narrative_notes_filter_column = ['narrativeNote']\n",
        "nursing_filter_column = ['assessment', 'diagnosisRelate', 'diagnosis', 'goal', 'plan', 'interventions', 'evaluation']\n",
        "\n",
        "nanda_df = function_apply(nanda_df, nanda_filter_column)\n",
        "soapie_df = function_apply(soapie_df, soapie_filter_column)\n",
        "focus_dar_df = function_apply(focus_dar_df, focus_dar_filter_column)\n",
        "narrative_notes_df = function_apply(narrative_notes_df, narrative_notes_filter_column)\n",
        "nursing_df = function_apply(nursing_df, nursing_filter_column)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nanda_df['input'] = nanda_df.apply(lambda x: f\"domain: {x['domain']},\\n\"\n",
        "                                  f\"class: {x['class']},\\n\"\n",
        "                                  f\"diagnosis: {x['diagnosis']},\\n\"\n",
        "                                  f\"collectingData: {x['collectingData']},\\n\"\n",
        "                                  f\"goal: {x['goal']},\\n\"\n",
        "                                  f\"plan: {x['plan']},\\n\"\n",
        "                                  f\"interventions: {x['interventions']},\\n\"\n",
        "                                  f\"evaluation: {x['evaluation']}\", axis=1)"
      ],
      "metadata": {
        "id": "mh0AQK9EhlEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soapie_df['input'] = soapie_df.apply(lambda x: f\"subjective: {x['subjective']},\\n\"\n",
        "                                  f\"objective: {x['objective']},\\n\"\n",
        "                                  f\"assessment: {x['assessment']},\\n\"\n",
        "                                  f\"planning: {x['planning']},\\n\"\n",
        "                                  f\"interventions: {x['interventions']},\\n\"\n",
        "                                  f\"evaluation: {x['evaluation']},\\n\", axis=1)"
      ],
      "metadata": {
        "id": "BAlwrghAhCGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "focus_dar_df['input'] = focus_dar_df.apply(lambda x: f\"focus: {x['focus']},\\n\"\n",
        "                                  f\"data: {x['data']},\\n\"\n",
        "                                  f\"action: {x['action']},\\n\"\n",
        "                                  f\"response: {x['response']},\\n\"\n",
        "                                  , axis=1)"
      ],
      "metadata": {
        "id": "zRHFl0bNhu1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nursing_df['input'] = nursing_df.apply(lambda x: f\"assessment: {x['assessment']},\\n\"\n",
        "                                  f\"diagnosisRelate: {x['diagnosisRelate']},\\n\"\n",
        "                                  f\"diagnosis: {x['diagnosis']},\\n\"\n",
        "                                  f\"goal: {x['goal']},\\n\"\n",
        "                                  f\"plan: {x['plan']},\\n\"\n",
        "                                  f\"interventions: {x['interventions']},\\n\"\n",
        "                                  f\"evaluation: {x['evaluation']},\\n\"\n",
        "                                  , axis=1)"
      ],
      "metadata": {
        "id": "LaEuAlVs4IiX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}